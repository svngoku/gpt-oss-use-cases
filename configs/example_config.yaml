# Example configuration for GPT OSS models

model:
  model_id: "gpt2"  # Can be any HuggingFace model ID
  device: "auto"
  dtype: "auto"
  cache_dir: "./models"
  
  # Optional quantization settings
  # quantization:
  #   load_in_4bit: true
  #   bnb_4bit_compute_dtype: "float16"
  #   bnb_4bit_quant_type: "nf4"
  #   bnb_4bit_use_double_quant: true

training:
  learning_rate: 5.0e-5
  batch_size: 4
  gradient_accumulation_steps: 2
  num_epochs: 3
  warmup_steps: 500
  save_steps: 1000
  eval_steps: 500
  output_dir: "./output"
  use_wandb: false

data:
  # Option 1: Use HuggingFace dataset
  dataset_name: "wikitext"
  
  # Option 2: Use local files
  # train_file: "./data/train.jsonl"
  # val_file: "./data/val.jsonl"
  # test_file: "./data/test.jsonl"
  
  max_length: 2048
  preprocessing_num_workers: 4

# General settings
seed: 42
experiment_name: "gpt_oss_experiment"
